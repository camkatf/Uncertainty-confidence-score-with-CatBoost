{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp m5.dataprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation & feature engineering\n",
    "\n",
    "> Description of all calculated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different kind of features will be computed here :\n",
    "\n",
    "   - Time series features\n",
    "   - Lags of the target variable (sales) and some variations of it (rolling means, aggregations)\n",
    "   - Pricing features\n",
    "   - Cumulative means of sales on different time frames and at various levels of aggregation\n",
    "   - Encoding for all categorical variables\n",
    "   \n",
    "More features than needed will be calculated, in order to be able to select the best afterwards. \n",
    "\n",
    "Before going into more details about each kind of predictor, the following chart shows the **best-performing features**, classified by category, based on the selection method that will be described later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: This chart only shows the best-performing features, therefore even the \"one star\" ones are among the most relevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import os\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "import logging\n",
    "from functools import reduce\n",
    "\n",
    "from functools import wraps\n",
    "import datetime as dt\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from copy import deepcopy\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following versions of libraries have been used :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas : 1.3.4\n",
      "Numpy : 1.20.3\n"
     ]
    }
   ],
   "source": [
    "print('Pandas :',pd.__version__)\n",
    "print('Numpy :',np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "class GCSConnector:\n",
    "    \"\"\"\n",
    "    Object: GCSConnector(Object)\n",
    "    Purpose: Connector to the GCS account\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bucketname, project_id):\n",
    "        \"\"\"\n",
    "        Initialize Google Cloud Storage Connector to bucket\n",
    "        :param1 bucketname: (str) bucket name\n",
    "        :param2 project_id: (str) projet id\n",
    "        \"\"\"\n",
    "        client = storage.Client(project=project_id)\n",
    "        self._bucket = client.get_bucket(bucketname)\n",
    "\n",
    "    def get_file(self, filename):\n",
    "        \"\"\"\n",
    "        Get file content from GCS\n",
    "        :param filename:\n",
    "        :return: (BytesIO) GCS File as byte\n",
    "        \"\"\"\n",
    "        blob = storage.Blob(filename, self._bucket)\n",
    "        content = blob.download_as_string()\n",
    "        return BytesIO(content)\n",
    "\n",
    "    def send_json(self, json_file, filename):\n",
    "        \"\"\"\n",
    "        :param json_file:\n",
    "        :param filename:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._bucket.blob(filename).upload_from_string(json.dumps(json_file, ensure_ascii=False))\n",
    "\n",
    "    def send_dataframe(self, df, filename, **kwargs):\n",
    "        \"\"\"\n",
    "        :param filename:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._bucket.blob(filename).upload_from_string(\n",
    "            df.to_csv(index=False, **kwargs), content_type=\"application/octet-stream\")\n",
    "\n",
    "    def open_csv_as_dataframe(self, filename, **kwargs):\n",
    "        \"\"\"\n",
    "        :param filename:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self.get_file(filename=filename), **kwargs)\n",
    "\n",
    "    def open_csv_as_dataframe_dtype(self, filename, dtypes_dict, **kwargs):\n",
    "        \"\"\"\n",
    "        :param filename:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self.get_file(filename=filename), dtype=dtypes_dict, **kwargs)\n",
    "\n",
    "    def open_json_as_dataframe(self, filename, **kwargs):\n",
    "        \"\"\"\n",
    "        :param filename:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return pd.read_json(self.get_file(filename=filename), **kwargs)\n",
    "\n",
    "    def open_excel_as_dataframe(self, filename, **kwargs):\n",
    "        \"\"\"\n",
    "        :param filename:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return pd.read_excel(self.get_file(filename=filename), **kwargs)\n",
    "\n",
    "    def file_exists(self, filename):\n",
    "        \"\"\"\n",
    "        Check if 'filename' file exists within bucket\n",
    "        :param filename:\n",
    "        :return: (Bool)\n",
    "        \"\"\"\n",
    "        return storage.Blob(filename, self._bucket).exists(self._gcsclient)\n",
    "\n",
    "    def list_files(self, prefix, delimiter=None):\n",
    "        return [blob.name for blob in self._bucket.list_blobs(prefix=prefix, delimiter=delimiter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Working locally or on a VM\n",
    "\n",
    "if os.getcwd().split('/')[1] == 'Users':  # Local\n",
    "    stored_locally = True\n",
    "    currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "    parentdir = os.path.dirname(currentdir)\n",
    "    main_path = parentdir + \"/data/\"\n",
    "    \n",
    "elif os.getcwd().split('/')[1] == 'home':  # VM\n",
    "    stored_locally = False\n",
    "    bucketname = \"m5-forecast\"\n",
    "    project_id = \"data-sandbox-fr\"\n",
    "    GCS_CONNECTOR = GCSConnector(bucketname, project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: where to find the data? \n",
    "# inside the project data-sandbox-fr on GCP, inside Google Cloud Storage\n",
    "# in the bucket m5-forecast/camille-uncertainty-with-catboost\n",
    "# you can use the folder raw_data\n",
    "\n",
    "# Input files path\n",
    "calendar_path = \"raw_data/calendar.csv\"\n",
    "sales_path = \"raw_data/sales_train_evaluation.csv\"\n",
    "prices_path = \"raw_data/sell_prices.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Cross-validation scheme\n",
    "\n",
    "END_PRED = 1968 \n",
    "START_PRED = END_PRED - 27\n",
    "DAYS_PRED = list(range(START_PRED, END_PRED + 1))\n",
    "\n",
    "END_VALID1 = START_PRED - 1\n",
    "START_VALID1 = END_VALID1 - 27\n",
    "DAYS_VALID1 = list(range(START_VALID1, END_VALID1 + 1))  # 28 days period before pred\n",
    "\n",
    "END_VALID2 = START_VALID1 - 1\n",
    "START_VALID2 = END_VALID2 - 27\n",
    "DAYS_VALID2 = list(range(START_VALID2, END_VALID2 + 1))  # 28 days period before\n",
    "\n",
    "END_VALID3 = START_VALID2 - 1\n",
    "START_VALID3 = END_VALID3 - 27\n",
    "DAYS_VALID3 = list(range(START_VALID3, END_VALID3 + 1))  # 28 days period before\n",
    "\n",
    "END_VALID4 = END_PRED - 366\n",
    "START_VALID4 = END_VALID4 - 27\n",
    "DAYS_VALID4 = list(range(START_VALID4, END_VALID4 + 1))  # 1 year before pred\n",
    "\n",
    "END_VALID5 = END_VALID4 - 365\n",
    "START_VALID5 = END_VALID5 - 27\n",
    "DAYS_VALID5 = list(range(START_VALID5, END_VALID5 + 1))  # 2 years before pred\n",
    "\n",
    "END_TRAIN1 = START_VALID1 - 1\n",
    "END_TRAIN2 = START_VALID2 - 1\n",
    "END_TRAIN3 = START_VALID3 - 1\n",
    "END_TRAIN4 = START_VALID4 - 1\n",
    "END_TRAIN5 = START_VALID5 - 1\n",
    "END_TRAIN_all = START_PRED - 1\n",
    "\n",
    "START_TRAIN = 0\n",
    "\n",
    "DAYS_TRAIN1 = list(range(START_TRAIN, END_TRAIN1 + 1))\n",
    "DAYS_TRAIN2 = list(range(START_TRAIN, END_TRAIN2 + 1))\n",
    "DAYS_TRAIN3 = list(range(START_TRAIN, END_TRAIN3 + 1))\n",
    "DAYS_TRAIN4 = list(range(START_TRAIN, END_TRAIN4 + 1))\n",
    "DAYS_TRAIN5 = list(range(START_TRAIN, END_TRAIN5 + 1))\n",
    "DAYS_TRAIN_all = list(range(START_TRAIN, END_TRAIN_all + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def log_step(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        tic = datetime.datetime.now()\n",
    "        result = func(*args, **kwargs)\n",
    "        time_taken = str(datetime.datetime.now() - tic)\n",
    "        logging.info(f\" -- Step {func.__name__} took {time_taken}s - {tic} --\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few functions will be needed to do all the feature engineering :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `reduce_mem_usage` optimizes columns types to reduce memory usage when computing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if str(col_type) in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem,\n",
    "                                                                              100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `merge_by_concat` allows to merge two dataframes without losing dtypes information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_df = df1[merge_on]\n",
    "    merged_df = merged_df.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_df) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_df[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing a group of new features, only some features computed previously need to be kept in memory (as they will be needed to calculate the new ones). All features not needed can be dropped. That's what the function `get_input_df` is used for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_input_df(input_cols):\n",
    "    index_cols = ['id','date_block_num_day']\n",
    "    input_df = features_time.copy()\n",
    "    return input_df[index_cols + input_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a group of features is calculated and stored in a dataframe, the function `feature_formatting` is used for :\n",
    " - Keeping only the columns needed\n",
    " - Optimizing memory usage \n",
    " - Printing features information (dtypes and NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def feature_formatting(df, input_cols):\n",
    "    index_cols = ['id','date_block_num_day']\n",
    "    df = df.drop(input_cols, axis=1)\n",
    "    cols_to_keep = index_cols + [i for i in df.columns if i not in index_cols]\n",
    "    df = df[cols_to_keep]\n",
    "    df = reduce_mem_usage(df)\n",
    "    print(df.drop(index_cols,axis=1).info(null_counts=True,memory_usage=False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of 3 files : sales, prices and calendar data. All input files can be found on [GCS](https://console.cloud.google.com/storage/browser/m5-forecast;tab=objects?forceOnBucketsSortingFiltering=false&project=data-sandbox-fr&prefix=). To execute this notebook and the following ones, put all the files in a folder named `data` located in the same directory as the notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv(f\"{sales_path}\")\n",
    "calendar_df = pd.read_csv(f\"{calendar_path}\")\n",
    "prices_df = pd.read_csv(f\"{prices_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sales data** contains historical sales count for the last 5 years, for each product in every location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "\n",
       "[5 rows x 1947 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calendar data** is composed of :\n",
    " - Date information\n",
    " - Calendar events\n",
    " - Snap days (days when people get benefits to buy food) for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>Monday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  wm_yr_wk    weekday  wday  month  year    d event_name_1  \\\n",
       "0  2011-01-29     11101   Saturday     1      1  2011  d_1          NaN   \n",
       "1  2011-01-30     11101     Sunday     2      1  2011  d_2          NaN   \n",
       "2  2011-01-31     11101     Monday     3      1  2011  d_3          NaN   \n",
       "3  2011-02-01     11101    Tuesday     4      2  2011  d_4          NaN   \n",
       "4  2011-02-02     11101  Wednesday     5      2  2011  d_5          NaN   \n",
       "\n",
       "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0          NaN          NaN          NaN        0        0        0  \n",
       "1          NaN          NaN          NaN        0        0        0  \n",
       "2          NaN          NaN          NaN        0        0        0  \n",
       "3          NaN          NaN          NaN        1        1        0  \n",
       "4          NaN          NaN          NaN        1        0        1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prices data** gives the historical price of each item in each store (prices can change every week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_id        item_id  wm_yr_wk  sell_price\n",
       "0     CA_1  HOBBIES_1_001     11325        9.58\n",
       "1     CA_1  HOBBIES_1_001     11326        9.58\n",
       "2     CA_1  HOBBIES_1_001     11327        8.26\n",
       "3     CA_1  HOBBIES_1_001     11328        8.26\n",
       "4     CA_1  HOBBIES_1_001     11329        8.26"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run everything quickly and avoid memory issues, we will work here on a subset (300 items out of the 3049 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = sales_df['item_id'].unique()\n",
    "subset_size = 300\n",
    "subset_items = np.random.choice(all_items,subset_size,replace=False)\n",
    "sales_df = sales_df.loc[sales_df['item_id'].isin(subset_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pre-processing is applied before calculating features :\n",
    "\n",
    " - Sales data is melted, to get one row per item x store and per day.\n",
    " - Rows for days to forecast are concatenated at the bottom of sales data, to compute features for these days as well later.\n",
    " - Calendar data, prices data and sales data are merged.\n",
    " - We don't want to consider rows where sales = 0 when this happens even before the product was released. So for each item x store, we calculate the date of the first sale of the product in the store (\"release\" date). Then we keep only the rows whose date is after the release date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def format_calendar_data(calendar):\n",
    "    calendar[\"date\"] = pd.to_datetime(calendar[\"date\"], format='%Y-%m-%d')\n",
    "    cols_calendar1 = ['date','d','wm_yr_wk']\n",
    "    cols_calendar2 = ['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\n",
    "    calendar = calendar[cols_calendar1 + cols_calendar2]\n",
    "    for col in cols_calendar2:\n",
    "        calendar[col] = calendar[col].astype('category')\n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def melt_sales_data(sales):\n",
    "    index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "    sales = sales.melt(id_vars=index_columns, var_name=\"date\", value_name=\"sales\")\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def add_forecast_days(sales, n_items=3049):\n",
    "    subset = sales.loc[sales['date'].isin(['d_' + str(i) for i in range(1,29)])].copy()\n",
    "    subset['date'] = np.ravel([['d_' + str(i)] * n_items*10 for i in range(1942,1970)])\n",
    "    subset['sales'] = np.nan\n",
    "    sales = pd.concat([sales,subset], axis=0).reset_index(drop=True)\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def reduce_memory(sales):\n",
    "    index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "    for col in index_columns:\n",
    "        sales[col] = sales[col].astype('category')\n",
    "    sales = reduce_mem_usage(sales)\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def create_release_date_column(sales,prices):\n",
    "    release_df = prices.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "    release_df.columns = ['store_id','item_id','release']\n",
    "    sales = merge_by_concat(sales, release_df, ['store_id','item_id'])\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def merge_sales_calendar(sales,calendar):\n",
    "    sales = sales.rename(columns={\"date\":\"d\"})\n",
    "    sales = merge_by_concat(sales, calendar, [\"d\"])\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def filter_out_sales_before_release_date(sales):\n",
    "    # Filter out rows for which date < item release date (not \"real zeros\")\n",
    "    sales = sales[sales['wm_yr_wk']>=sales['release']]\n",
    "    sales = sales.reset_index(drop=True)\n",
    "    # Normalize release column\n",
    "    sales['release'] = sales['release'] - sales['release'].min()\n",
    "    sales['release'] = sales['release'].astype(np.int16)\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def merge_sales_prices(sales,prices):\n",
    "    sales = merge_by_concat(sales, prices, [\"store_id\",\"item_id\",\"wm_yr_wk\"])\n",
    "    sales = sales.drop([\"d\"], axis=1)\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step melt_sales_data took 0:00:02.651604s - 2022-05-24 15:08:28.280837 --\n",
      "INFO:root: -- Step add_forecast_days took 0:00:02.602924s - 2022-05-24 15:08:30.936186 --\n",
      "INFO:root: -- Step reduce_memory took 0:00:04.602021s - 2022-05-24 15:08:33.686497 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 101.50 Mb (25.0% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step create_release_date_column took 0:00:03.604285s - 2022-05-24 15:08:38.289312 --\n",
      "INFO:root: -- Step format_calendar_data took 0:00:00.037030s - 2022-05-24 15:08:41.931631 --\n",
      "INFO:root: -- Step merge_sales_calendar took 0:00:01.678634s - 2022-05-24 15:08:41.969969 --\n",
      "INFO:root: -- Step filter_out_sales_before_release_date took 0:00:00.791945s - 2022-05-24 15:08:43.696295 --\n",
      "INFO:root: -- Step merge_sales_prices took 0:00:05.371927s - 2022-05-24 15:08:44.559812 --\n"
     ]
    }
   ],
   "source": [
    "sales_df = sales_df.pipe(melt_sales_data) \\\n",
    "                   .pipe(add_forecast_days, n_items = subset_size) \\\n",
    "                   .pipe(reduce_memory) \\\n",
    "                   .pipe(create_release_date_column, prices = prices_df) \\\n",
    "                   .pipe(merge_sales_calendar, calendar = format_calendar_data(calendar_df)) \\\n",
    "                   .pipe(filter_out_sales_before_release_date) \\\n",
    "                   .pipe(merge_sales_prices, prices = prices_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready for feature calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4652320, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>release</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_015_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_015</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_029_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_029</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_036_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_036</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_044_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_044</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_055_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_055</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_015_CA_1_evaluation  HOBBIES_1_015  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_029_CA_1_evaluation  HOBBIES_1_029  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_036_CA_1_evaluation  HOBBIES_1_036  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_044_CA_1_evaluation  HOBBIES_1_044  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_055_CA_1_evaluation  HOBBIES_1_055  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  sales  release       date  wm_yr_wk event_name_1 event_type_1  \\\n",
       "0       CA    4.0        0 2011-01-29     11101          NaN          NaN   \n",
       "1       CA    2.0        0 2011-01-29     11101          NaN          NaN   \n",
       "2       CA    2.0        0 2011-01-29     11101          NaN          NaN   \n",
       "3       CA    3.0        0 2011-01-29     11101          NaN          NaN   \n",
       "4       CA    0.0        0 2011-01-29     11101          NaN          NaN   \n",
       "\n",
       "  event_name_2 event_type_2 snap_CA snap_TX snap_WI  sell_price  \n",
       "0          NaN          NaN       0       0       0        0.70  \n",
       "1          NaN          NaN       0       0       0        7.44  \n",
       "2          NaN          NaN       0       0       0        0.96  \n",
       "3          NaN          NaN       0       0       0        1.51  \n",
       "4          NaN          NaN       0       0       0        7.44  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sales_df.shape)\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "del calendar_df\n",
    "del prices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to compute a large number of features among those mentioned in the Kaggle discussions. These features won't be all kept for predictions, they will be selected later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute classical time features : day, week, month, year..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@log_step\n",
    "def time_series_features(df, name_date_col):\n",
    "\n",
    "#    Function calculating some feature related to the date.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (name_date_col): the name of date column\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.   \n",
    "\n",
    "    df[name_date_col] = pd.to_datetime(df[name_date_col], format='%Y-%m-%d')\n",
    "\n",
    "    if df[name_date_col].dtype != '<M8[ns]': # FONCTION PANDAS ERREUR SI PAS DATETIME\n",
    "        raise ValueError('{} must be a datetime column '.format(name_date_col))\n",
    "    \n",
    "    df[\"day\"] = df[name_date_col].dt.day.astype(np.int8)\n",
    "    df['week'] = df[name_date_col].dt.week.astype(np.int8)\n",
    "    df[\"month\"] = df[name_date_col].dt.month.astype(np.int8)\n",
    "    df[\"year\"] = df[name_date_col].dt.year\n",
    "    \n",
    "    df[\"dayofweek\"] = df[name_date_col].dt.dayofweek.astype(np.int8)\n",
    "    df['weekend'] = (df[\"dayofweek\"]>=5).astype(np.int8)\n",
    "    df[\"dayofyear\"] = df[name_date_col].dt.dayofyear.astype(np.int16)\n",
    "\n",
    "    # Be careful to have the same min_date in your train, test dataset.\n",
    "    # Especially in production if you don't train and predict at the same moment.\n",
    "    first_day = df[name_date_col].min()\n",
    "\n",
    "    df[\"date_block_num_month\"] = ((df[name_date_col] - first_day) / np.timedelta64(1, 'M'))\n",
    "    df[\"date_block_num_week\"] = ((df[name_date_col] - first_day) / np.timedelta64(1, 'W'))\n",
    "    df[\"date_block_num_day\"] = ((df[name_date_col] - first_day) / np.timedelta64(1, 'D'))\n",
    "    \n",
    "    df['date_block_num_month'] = df['date_block_num_month'].astype(int)\n",
    "    df['date_block_num_week'] = df['date_block_num_week'].astype(int)\n",
    "    df['date_block_num_day'] = df['date_block_num_day'].astype(int)\n",
    "    \n",
    "    df[\"day_temp\"] = 1\n",
    "    df[\"year_month\"] = pd.to_datetime(dict(year=df.year, month=df.month, day=df.day_temp))\n",
    "    df = df.drop(\"day_temp\",axis=1)\n",
    "    \n",
    "    df[\"year\"] = (df[\"year\"] - df[\"year\"].min()).astype(np.int8)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step time_series_features took 0:00:05.662759s - 2022-05-24 15:08:51.367356 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 230.81 Mb (37.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "input_df = sales_df.copy()\n",
    "\n",
    "features_time = input_df.pipe(time_series_features, 'date') \\\n",
    "                        .pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4652320 entries, 0 to 4652319\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count    Dtype         \n",
      "---  ------                --------------    -----         \n",
      " 0   year_month            4652320 non-null  datetime64[ns]\n",
      " 1   month                 4652320 non-null  int8          \n",
      " 2   date_block_num_day    4652320 non-null  int16         \n",
      " 3   weekend               4652320 non-null  int8          \n",
      " 4   date_block_num_week   4652320 non-null  int16         \n",
      " 5   week                  4652320 non-null  int8          \n",
      " 6   dayofweek             4652320 non-null  int8          \n",
      " 7   dayofyear             4652320 non-null  int16         \n",
      " 8   date_block_num_month  4652320 non-null  int8          \n",
      " 9   day                   4652320 non-null  int8          \n",
      " 10  year                  4652320 non-null  int8          \n",
      "dtypes: datetime64[ns](1), int16(3), int8(7)"
     ]
    }
   ],
   "source": [
    "# Display features just calculated, with type and NaN count\n",
    "features_time[set(features_time.columns) - set(sales_df.columns)].info(null_counts=True,memory_usage=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative means of sales in similar periods in the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compute cumulative averages of sales over similar periods back in time, at different levels of aggregation.\n",
    "\n",
    "The periods considered are : same week (1-52) / same day of the month (1-31) / same month (1-12), and the aggregation levels are defined in the `list_agg_levels` list below. \n",
    "\n",
    "For example, if a given row corresponds to the sales of product **HOBBIES_1_004** in store **CA_1** on a day of **week 33** of **year 2016**, the feature `cum_mean_week_store_id_item_id` will be equal to the average sales of the same product in the same store **over weeks 33 of years 2011 to 2015**. The current year (2016 here) is excluded from the average because it won't be available on the prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cum_mean_similar_periods(df, period:str, agg_level:list, target_name:str):\n",
    "\n",
    "#    Function calculating the cumulative mean of sales at a given level of aggregation,\n",
    "#    on similar periods in the past (ex: same week on previous years)\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (period): name of the period on which cumulative means will be computed: \n",
    "#                            - 'day' : same day of the month\n",
    "#                            - 'week' : same week of the year\n",
    "#                            - 'month' : same month of the year\n",
    "#        param3 (agg_level): list of column names to define the aggregation level\n",
    "#        param4 (target_name): name of the target\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.\n",
    "    \n",
    "    def sum_div1000(x):\n",
    "        # This function is only used to avoid getting \"inf\" when sums become too large\n",
    "        return 0.001 * sum(x)\n",
    "    \n",
    "    agg_cols = [period] + agg_level\n",
    "    group = df.groupby(['year'] + agg_cols)[target_name].agg(['count',sum_div1000]).reset_index().dropna(subset=['sum_div1000'])\n",
    "    cumsums = group.groupby(agg_cols).cumsum()[['count','sum_div1000']]\n",
    "    cumsums.columns = ['cum_count','cum_sum']\n",
    "    group2 = group.merge(cumsums, how='left', left_index=True, right_index=True)\n",
    "    group2['cum_mean_' + '_'.join(agg_cols)] = 1000 * group2['cum_sum'] / group2['cum_count']\n",
    "    group2 = group2[['year'] + agg_cols + ['cum_mean_' + '_'.join(agg_cols)]]\n",
    "    group2['year'] = group2['year'].map(lambda x : x+1)\n",
    "    \n",
    "    df = df.merge(group2, how='left', on=['year']+agg_cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def cum_mean_similar_periods_calculation(input_df, list_periods, list_agg_levels, target_name):\n",
    "    df = input_df.copy()\n",
    "    for period in list_periods:\n",
    "        for agg_level in list_agg_levels:\n",
    "            df = cum_mean_similar_periods(df, period, agg_level, target_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME = 'sales'\n",
    "list_periods = ['week','day','month']\n",
    "list_agg_levels = [   ['item_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step cum_mean_similar_periods_calculation took 0:00:12.320478s - 2022-05-24 15:08:59.635713 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 79.95 Mb (50.0% reduction)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4652320 entries, 0 to 4652319\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   cum_mean_week_item_id   3694378 non-null  float16\n",
      " 1   cum_mean_day_item_id    3907936 non-null  float16\n",
      " 2   cum_mean_month_item_id  3740233 non-null  float16\n",
      "dtypes: float16(3)None\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['sales','year','week','day','month','dayofyear','state_id','store_id','cat_id','dept_id','item_id']\n",
    "input_df = get_input_df(input_cols)\n",
    "\n",
    "features_cum_mean = input_df.pipe(cum_mean_similar_periods_calculation, list_periods, list_agg_levels, TARGET_NAME) \\\n",
    "                            .pipe(feature_formatting, input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pricing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute pricing features of different kinds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Relative price difference within a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pricing_features_1(df_i, list_col_agg:list, target_var:str, output_var:str):\n",
    "\n",
    "#    Function calculating the price relative difference of products within a group.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (list_col_agg): the list of columns you want to apply the aggregagtion on\n",
    "#        param3 (target_var): the price column on which to calculate the relative difference\n",
    "#        param4 (output_var): output column name\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.\n",
    "\n",
    "    df_agg = df_i.groupby(list_col_agg).agg({target_var:np.nanmean}).reset_index()\n",
    "    df_agg = df_agg.rename(columns={target_var: \"avg_target\"})\n",
    "    df_i = pd.merge(df_i, df_agg, on=list_col_agg, how='left')\n",
    "    df_i[output_var] = np.where(df_i[\"avg_target\"] == 0\\\n",
    "                                ,0\\\n",
    "                                ,(df_i[target_var]-df_i[\"avg_target\"])/df_i[\"avg_target\"])\n",
    "    df_i = df_i.drop([\"avg_target\"],axis=1)\n",
    "\n",
    "    return df_i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def pricing_features_1_calculation(input_df, list_dict):\n",
    "    df = input_df.copy()\n",
    "    for dict_i in list_dict:\n",
    "        df = pricing_features_1(df_i = df,\n",
    "                                      list_col_agg = dict_i[\"list_col_agg\"],\n",
    "                                      target_var = dict_i[\"target_var\"],\n",
    "                                      output_var = dict_i[\"output_var\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_1 = [   {\"list_col_agg\":[\"year_month\",\"item_id\"]\n",
    "                  ,\"target_var\":\"sell_price\"\n",
    "                  ,\"output_var\":\"diff_price_same_month_item\"}\n",
    "\n",
    "                 ,{\"list_col_agg\":[\"date\",\"item_id\"]\n",
    "                  ,\"target_var\":\"sell_price\"\n",
    "                  ,\"output_var\":\"diff_price_same_day_item\"}\n",
    "\n",
    "                 ,{\"list_col_agg\":[\"item_id\"]\n",
    "                  ,\"target_var\":\"sell_price\"\n",
    "                  ,\"output_var\":\"diff_price_same_item\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Simple aggregations, at the item x store level : min, max, mean, std, nunique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pricing_features_2(df_i, list_col_agg: list, aggregator, target_var: str, output_var: str):\n",
    "\n",
    "#    Function calculating basic aggregation statistics on prices.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (list_col_agg): the list of columns you want to apply the aggregagtion on\n",
    "#        param3 (aggregator): the aggregation function (min, max, std, mean, median...)\n",
    "#        param4 (target_var): the price column you want to aggregate\n",
    "#        param5 (output_var): output column name\n",
    "#    Returns:\n",
    "#       df: the input dataframe with nice features.\n",
    "\n",
    "    df_agg = df_i.groupby(list_col_agg).agg({target_var:aggregator}).reset_index()\n",
    "    df_agg = df_agg.rename(columns={target_var:output_var})\n",
    "    df_i = pd.merge(df_i, df_agg, on=list_col_agg, how='left')\n",
    "\n",
    "    return df_i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def pricing_features_2_calculation(df, list_dict):\n",
    "    for dict_i in list_dict:\n",
    "        df = pricing_features_2(df_i=df,\n",
    "                                aggregator=dict_i[\"aggregator\"],\n",
    "                                list_col_agg=dict_i[\"list_col_agg\"],\n",
    "                                target_var=dict_i[\"target_var\"],\n",
    "                                output_var=dict_i[\"output_var\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def nunique(liste):\n",
    "    return len(np.unique(liste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_2 = [   {\"list_col_agg\":[\"store_id\",\"item_id\"]\n",
    "                  ,\"aggregator\": np.nanmin\n",
    "                  ,\"target_var\":\"sell_price\"\n",
    "                  ,\"output_var\":\"min_price_same_store_item\"}\n",
    "\n",
    "                 ,{\"list_col_agg\":[\"store_id\",\"item_id\"]\n",
    "                  ,\"aggregator\": np.nanmax\n",
    "                  ,\"target_var\":\"sell_price\"\n",
    "                  ,\"output_var\":\"max_price_same_store_item\"}\n",
    "\n",
    "                 ,{\"list_col_agg\":[\"store_id\",\"item_id\"]\n",
    "                  ,\"aggregator\": np.nanmean\n",
    "                  ,\"target_var\":\"sell_price\"\n",
    "                  ,\"output_var\":\"mean_price_same_store_item\"}\n",
    "\n",
    "                 ,{\"list_col_agg\":[\"store_id\",\"item_id\"]\n",
    "                  ,\"aggregator\": np.nanstd\n",
    "                  ,\"target_var\":\"sell_price\"\n",
    "                  ,\"output_var\":\"std_price_same_store_item\"}\n",
    "\n",
    "                 ,{\"list_col_agg\":[\"store_id\",\"item_id\"]\n",
    "                  ,\"aggregator\": nunique\n",
    "                  ,\"target_var\":\"sell_price\"\n",
    "                  ,\"output_var\":\"nunique_price_same_store_item\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Price momentum and normalized price, which are among the price features that perform the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def pricing_features_3(df, prices, calendar):\n",
    "\n",
    "#    Function calculating price momentum and normalized price.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (prices): input dataframe prices_df\n",
    "#        param3 (calendar): input dataframe calendar_df\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.\n",
    "    \n",
    "    # Price max normalization\n",
    "    df['price_norm'] = df['sell_price'] / df['max_price_same_store_item']\n",
    "    \n",
    "    # Import calendar and prices dataframes\n",
    "    calendar_prices = calendar[['wm_yr_wk','month','year']].drop_duplicates(subset=['wm_yr_wk'])\n",
    "    prices_df = prices.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "    \n",
    "    # Price momentum : kind of price derivative\n",
    "    prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    \n",
    "    # Ratio / average monthly price by item x store\n",
    "    prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "    \n",
    "    # Ratio / average yearly price by item x store\n",
    "    prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "    \n",
    "    # Merge everything together\n",
    "    prices_df = prices_df[[\"store_id\",\"item_id\",\"wm_yr_wk\",\"price_momentum\",\"price_momentum_m\",\"price_momentum_y\"]]\n",
    "    df = merge_by_concat(df, prices_df, [\"store_id\",\"item_id\",\"wm_yr_wk\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate all these features successively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step pricing_features_1_calculation took 0:00:24.323651s - 2022-05-24 15:09:18.482951 --\n",
      "INFO:root: -- Step pricing_features_2_calculation took 0:00:11.795279s - 2022-05-24 15:09:42.809812 --\n",
      "INFO:root: -- Step pricing_features_3 took 0:00:28.303708s - 2022-05-24 15:09:57.793880 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 155.37 Mb (47.0% reduction)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4652320 entries, 0 to 4652319\n",
      "Data columns (total 12 columns):\n",
      " #   Column                         Non-Null Count    Dtype  \n",
      "---  ------                         --------------    -----  \n",
      " 0   diff_price_same_month_item     4652320 non-null  float16\n",
      " 1   diff_price_same_day_item       4652320 non-null  float16\n",
      " 2   diff_price_same_item           4652320 non-null  float16\n",
      " 3   min_price_same_store_item      4652320 non-null  float16\n",
      " 4   max_price_same_store_item      4652320 non-null  float16\n",
      " 5   mean_price_same_store_item     4652320 non-null  float16\n",
      " 6   std_price_same_store_item      4652320 non-null  float16\n",
      " 7   nunique_price_same_store_item  4652320 non-null  int8   \n",
      " 8   price_norm                     4652320 non-null  float16\n",
      " 9   price_momentum                 4631320 non-null  float16\n",
      " 10  price_momentum_m               4652320 non-null  float16\n",
      " 11  price_momentum_y               4652320 non-null  float16\n",
      "dtypes: float16(11), int8(1)None\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['sell_price','date','year_month','wm_yr_wk','store_id','cat_id','dept_id','item_id']\n",
    "input_df = get_input_df(input_cols)\n",
    "\n",
    "features_prices = input_df.pipe(pricing_features_1_calculation, list_dict_1) \\\n",
    "                          .pipe(pricing_features_2_calculation, list_dict_2) \\\n",
    "                          .pipe(pricing_features_3, prices = pd.read_csv(f\"{prices_path}\"),\n",
    "                                                    calendar = pd.read_csv(f\"{calendar_path}\")) \\\n",
    "                          .pipe(feature_formatting, input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lags of target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lags of the target variable are computed :\n",
    " - Days : 8, 9, 10, ..., 35\n",
    " - Years : 1\n",
    " \n",
    "Some rolling means are also calculated :\n",
    "- 8-14 days, 8-21 days, 8-28 days, 8-35 days\n",
    "- 15-22 days, 15-29 days, 15-36 days, 15-43 days\n",
    "- 22-29 days, 22-36 days, 22-43 days, 22-50 days\n",
    "- 29-35 days, 29-42 days, 29-49 days, 29-56 days\n",
    "\n",
    "And some rolling standard deviations as well :\n",
    "- 8-14 days, 8-35 days\n",
    "- 15-22 days, 15-43 days\n",
    "- 22-29 days, 22-50 days\n",
    "- 29-35 days, 29-56 days\n",
    "\n",
    "Lags from day 1 to day 7 are not calculated because the prediction horizon will be at least one week, therefore these features wouldn't be possible to get for the prediction set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def lags_features(df, merge_cols:list, lags:list, target_var:str, lag_type:str):\n",
    "\n",
    "#    Function calculating the lags features of a given column.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (merge_cols): the list of columns you want to apply the aggregagtion on\n",
    "#        param3 (lags): the list of lags being integers\n",
    "#        param4 (target_var): the columns you want to apply the lags on\n",
    "#        param5 (lag_type): lag_type directly related to the datetime.timedelta function\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.\n",
    "\n",
    "    if not isinstance(merge_cols, list):\n",
    "        raise ValueError('merge_cols must be a list.')\n",
    "    if not isinstance(lags, list):\n",
    "        raise ValueError('lags must be a list.')\n",
    "    if lag_type not in ['days', 'weeks', 'months']:\n",
    "        raise ValueError('lag_type is not valid, either \"days\", \"weeks\" or \"months\" ')\n",
    "    \n",
    "    tmp = df[merge_cols + [target_var]]\n",
    "    \n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = merge_cols + [target_var + '_lag_{}_'.format(lag_type) + str(i)]\n",
    "        shifted['date'] += datetime.timedelta(**{lag_type: i})\n",
    "        df = pd.merge(df, shifted, on=merge_cols, how='left')\n",
    "        df[target_var + '_lag_{}_'.format(lag_type) + str(i)] = df[target_var + '_lag_{}_'.format(lag_type) + str(i)].fillna(0).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def rolling_lags_features(df, lag_type:str, days_origin:list, target_var:str):\n",
    "\n",
    "#    Function calculating rolling lags.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe with lags features already calculated.\n",
    "#        param2 (lag_type): type of lags considered (directly related to the datetime.timedelta function)\n",
    "#        param3 (days_origin): list of days from which rolling periods start\n",
    "#        param4 (target_var): the name of the target variable, on which lags are calculated\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.\n",
    "\n",
    "    for day_origin in days_origin:\n",
    "    \n",
    "        rolling_period_list = [day_origin+6,day_origin+13,day_origin+20,day_origin+27]\n",
    "\n",
    "        # Mean\n",
    "        for rolling_period in rolling_period_list:\n",
    "            col_name = f\"mean_{day_origin}-{rolling_period}_{lag_type}_{target_var}\"\n",
    "            list_lags = []\n",
    "            for i in range(day_origin,rolling_period+1):\n",
    "                list_lags.append(f\"{target_var}_lag_{lag_type}_{i}\")\n",
    "            df[col_name] = df[list_lags].mean(axis=1)\n",
    "\n",
    "        # Std\n",
    "        for rolling_period in [day_origin+6,day_origin+27]:\n",
    "            col_name = f\"std_{day_origin}-{rolling_period}_{lag_type}_{target_var}\"\n",
    "            list_lags = []\n",
    "            for i in range(day_origin,rolling_period+1):\n",
    "                list_lags.append(f\"{target_var}_lag_{lag_type}_{i}\")\n",
    "            df[col_name] = df[list_lags].std(axis=1)\n",
    "            \n",
    "    # Remove some individual lags that are not needed\n",
    "    cols_to_remove = [f\"sales_lag_days_{i}\" for i in range(36,57)]\n",
    "    df = df.drop(cols_to_remove, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGE_COLS = ['date','store_id','item_id']\n",
    "LAGS = list(range(8,57)) + [365,366]\n",
    "TARGET_NAME = 'sales'\n",
    "LAG_TYPE = 'days'\n",
    "DAYS_ORIGIN = [8,15,22,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step lags_features took 0:07:09.997227s - 2022-05-24 15:10:32.372023 --\n",
      "INFO:root: -- Step rolling_lags_features took 0:07:46.457791s - 2022-05-24 15:17:42.593051 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 532.50 Mb (73.0% reduction)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4652320 entries, 0 to 4652319\n",
      "Data columns (total 54 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   sales_lag_days_8       4652320 non-null  int16  \n",
      " 1   sales_lag_days_9       4652320 non-null  int16  \n",
      " 2   sales_lag_days_10      4652320 non-null  int16  \n",
      " 3   sales_lag_days_11      4652320 non-null  int16  \n",
      " 4   sales_lag_days_12      4652320 non-null  int16  \n",
      " 5   sales_lag_days_13      4652320 non-null  int16  \n",
      " 6   sales_lag_days_14      4652320 non-null  int16  \n",
      " 7   sales_lag_days_15      4652320 non-null  int16  \n",
      " 8   sales_lag_days_16      4652320 non-null  int16  \n",
      " 9   sales_lag_days_17      4652320 non-null  int16  \n",
      " 10  sales_lag_days_18      4652320 non-null  int16  \n",
      " 11  sales_lag_days_19      4652320 non-null  int16  \n",
      " 12  sales_lag_days_20      4652320 non-null  int16  \n",
      " 13  sales_lag_days_21      4652320 non-null  int16  \n",
      " 14  sales_lag_days_22      4652320 non-null  int16  \n",
      " 15  sales_lag_days_23      4652320 non-null  int16  \n",
      " 16  sales_lag_days_24      4652320 non-null  int16  \n",
      " 17  sales_lag_days_25      4652320 non-null  int16  \n",
      " 18  sales_lag_days_26      4652320 non-null  int16  \n",
      " 19  sales_lag_days_27      4652320 non-null  int16  \n",
      " 20  sales_lag_days_28      4652320 non-null  int16  \n",
      " 21  sales_lag_days_29      4652320 non-null  int16  \n",
      " 22  sales_lag_days_30      4652320 non-null  int16  \n",
      " 23  sales_lag_days_31      4652320 non-null  int16  \n",
      " 24  sales_lag_days_32      4652320 non-null  int16  \n",
      " 25  sales_lag_days_33      4652320 non-null  int16  \n",
      " 26  sales_lag_days_34      4652320 non-null  int16  \n",
      " 27  sales_lag_days_35      4652320 non-null  int16  \n",
      " 28  sales_lag_days_365     4652320 non-null  int16  \n",
      " 29  sales_lag_days_366     4652320 non-null  int16  \n",
      " 30  mean_8-14_days_sales   4652320 non-null  float16\n",
      " 31  mean_8-21_days_sales   4652320 non-null  float16\n",
      " 32  mean_8-28_days_sales   4652320 non-null  float16\n",
      " 33  mean_8-35_days_sales   4652320 non-null  float16\n",
      " 34  std_8-14_days_sales    4652320 non-null  float16\n",
      " 35  std_8-35_days_sales    4652320 non-null  float16\n",
      " 36  mean_15-21_days_sales  4652320 non-null  float16\n",
      " 37  mean_15-28_days_sales  4652320 non-null  float16\n",
      " 38  mean_15-35_days_sales  4652320 non-null  float16\n",
      " 39  mean_15-42_days_sales  4652320 non-null  float16\n",
      " 40  std_15-21_days_sales   4652320 non-null  float16\n",
      " 41  std_15-42_days_sales   4652320 non-null  float16\n",
      " 42  mean_22-28_days_sales  4652320 non-null  float16\n",
      " 43  mean_22-35_days_sales  4652320 non-null  float16\n",
      " 44  mean_22-42_days_sales  4652320 non-null  float16\n",
      " 45  mean_22-49_days_sales  4652320 non-null  float16\n",
      " 46  std_22-28_days_sales   4652320 non-null  float16\n",
      " 47  std_22-49_days_sales   4652320 non-null  float16\n",
      " 48  mean_29-35_days_sales  4652320 non-null  float16\n",
      " 49  mean_29-42_days_sales  4652320 non-null  float16\n",
      " 50  mean_29-49_days_sales  4652320 non-null  float16\n",
      " 51  mean_29-56_days_sales  4652320 non-null  float16\n",
      " 52  std_29-35_days_sales   4652320 non-null  float16\n",
      " 53  std_29-56_days_sales   4652320 non-null  float16\n",
      "dtypes: float16(24), int16(30)None\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['sales','date','store_id','item_id']\n",
    "input_df = get_input_df(input_cols)\n",
    "\n",
    "features_lags = input_df.pipe(lags_features, MERGE_COLS, LAGS, TARGET_NAME, LAG_TYPE) \\\n",
    "                        .pipe(rolling_lags_features, LAG_TYPE, DAYS_ORIGIN, TARGET_NAME) \\\n",
    "                        .pipe(feature_formatting, input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lags of target variable aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also aggregate sales by day at different levels (store, item, category...) and then compute lags of these aggregated sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lags_features_agg(df, target_name:str, avg_col_list:list, col_name:str, merge_cols:list, list_lags:list, lag_type:str):\n",
    "\n",
    "#    Function calculating lags of aggregated values of the target variable.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (target_name): the name of the column you want to do the avg on\n",
    "#        param2 (avg_col_list): the agg cols to use to do the avg\n",
    "#        param4 (col_name): the name of the output column\n",
    "#        param5 (merge_cols): the columns to apply the lags to\n",
    "#        param6 (list_lags): the list of lags being integers\n",
    "#        param7 (lag_type): lag_type directly related to the datetime.timedelta function\n",
    "#    Returns:\n",
    "#       df: the input dataframe with nice features.\n",
    "\n",
    "    if not isinstance(avg_col_list, list):\n",
    "        raise ValueError('avg_col_list must be a list.')\n",
    "    if not isinstance(merge_cols, list):\n",
    "        raise ValueError('merge_cols must be a list.')\n",
    "    if not isinstance(list_lags, list):\n",
    "        raise ValueError('list_lags must be a list.')\n",
    "    if lag_type not in ['days', 'weeks', 'months']:\n",
    "        raise ValueError('lag_type is not valid, either \"days\", \"weeks\" or \"months\".')\n",
    "\n",
    "    group = df.groupby(avg_col_list).agg({target_name: ['mean']})\n",
    "    group.columns = [col_name]\n",
    "    group = group.reset_index()\n",
    "\n",
    "    df = pd.merge(df, group, on=avg_col_list, how='left')\n",
    "    df[col_name] = df[col_name].astype(np.float16)\n",
    "\n",
    "    df = lags_features(df, merge_cols, list_lags, col_name, lag_type)\n",
    "    df = df.drop([col_name], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def lags_features_agg_calculation(input_df, target_name, merge_cols, lags, lag_type, list_dict):\n",
    "    df = input_df.copy()\n",
    "    for dict_i in list_dict:\n",
    "        df = lags_features_agg(df = df,\n",
    "                               target_name = target_name,\n",
    "                               avg_col_list = dict_i['avg_col_list'],\n",
    "                               col_name = dict_i['col_name'],\n",
    "                               merge_cols = merge_cols,\n",
    "                               list_lags = lags,\n",
    "                               lag_type = lag_type)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME = 'sales'\n",
    "MERGE_COLS = ['date','store_id','item_id']\n",
    "LAGS = [1,8,15,22,29,100,365]\n",
    "LAG_TYPE = 'days'\n",
    "\n",
    "list_dict = [{\"avg_col_list\":['date_block_num_day'],\n",
    "              \"col_name\":\"date_avg_sales\"},\n",
    "\n",
    "              {\"avg_col_list\":['date_block_num_day','store_id'],\n",
    "               \"col_name\":\"date_store_avg_sales\"},\n",
    "\n",
    "              {\"avg_col_list\":['date_block_num_day','item_id'],\n",
    "               \"col_name\":'date_item_avg_sales'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step lags_features took 0:01:20.055982s - 2022-05-24 15:30:38.695312 --\n",
      "INFO:root: -- Step lags_features took 0:01:14.642291s - 2022-05-24 15:32:04.958726 --\n",
      "INFO:root: -- Step lags_features took 0:02:50.615622s - 2022-05-24 15:33:49.948290 --\n",
      "INFO:root: -- Step lags_features_agg_calculation took 0:06:14.727996s - 2022-05-24 15:30:31.921068 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 146.50 Mb (81.7% reduction)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4652320 entries, 0 to 4652319\n",
      "Data columns (total 21 columns):\n",
      " #   Column                             Non-Null Count    Dtype\n",
      "---  ------                             --------------    -----\n",
      " 0   date_avg_sales_lag_days_1          4652320 non-null  int8 \n",
      " 1   date_avg_sales_lag_days_8          4652320 non-null  int8 \n",
      " 2   date_avg_sales_lag_days_15         4652320 non-null  int8 \n",
      " 3   date_avg_sales_lag_days_22         4652320 non-null  int8 \n",
      " 4   date_avg_sales_lag_days_29         4652320 non-null  int8 \n",
      " 5   date_avg_sales_lag_days_100        4652320 non-null  int8 \n",
      " 6   date_avg_sales_lag_days_365        4652320 non-null  int8 \n",
      " 7   date_store_avg_sales_lag_days_1    4652320 non-null  int8 \n",
      " 8   date_store_avg_sales_lag_days_8    4652320 non-null  int8 \n",
      " 9   date_store_avg_sales_lag_days_15   4652320 non-null  int8 \n",
      " 10  date_store_avg_sales_lag_days_22   4652320 non-null  int8 \n",
      " 11  date_store_avg_sales_lag_days_29   4652320 non-null  int8 \n",
      " 12  date_store_avg_sales_lag_days_100  4652320 non-null  int8 \n",
      " 13  date_store_avg_sales_lag_days_365  4652320 non-null  int8 \n",
      " 14  date_item_avg_sales_lag_days_1     4652320 non-null  int8 \n",
      " 15  date_item_avg_sales_lag_days_8     4652320 non-null  int8 \n",
      " 16  date_item_avg_sales_lag_days_15    4652320 non-null  int8 \n",
      " 17  date_item_avg_sales_lag_days_22    4652320 non-null  int8 \n",
      " 18  date_item_avg_sales_lag_days_29    4652320 non-null  int8 \n",
      " 19  date_item_avg_sales_lag_days_100   4652320 non-null  int8 \n",
      " 20  date_item_avg_sales_lag_days_365   4652320 non-null  int8 \n",
      "dtypes: int8(21)None\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['sales','date','store_id','item_id','dept_id','cat_id','state_id']\n",
    "input_df = get_input_df(input_cols)\n",
    "\n",
    "features_lags_agg = input_df.pipe(lags_features_agg_calculation, TARGET_NAME, MERGE_COLS, LAGS, LAG_TYPE, list_dict) \\\n",
    "                            .pipe(feature_formatting, input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoding for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features in the dataset need to be encoded (except if a catboost model is used). As some of them have a high cardinality (like item_id that can take more than 3000 distinct values), a one-hot encoding is not the prefered solution. \n",
    "\n",
    "A target encoding will be used instead : each modality of a given categorical feature is encoded by taking the average (or standard deviation) of sales among all rows of that modality. To avoid leakage, rows that will be used for validation or prediction are not considered when computing the mean or the std.\n",
    "\n",
    "All the categorical features will be encoded, and some combinations of them as well (combinations defined in the `to_encode` list below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def target_encoding(df, target_name:str, cols_to_encode:list, encoding_func:list, exclude_column:str, exclude_values:list):\n",
    "\n",
    "#    Function encoding a categorical feature (or a group of categorical features) based on aggregated target values.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (target_name): name of target column\n",
    "#        param3 (cols_to_encode): list of the names of the columns to encode\n",
    "#        param4 (encoding_func): list of the functions to use for encoding\n",
    "#        param5 (exclude_column): name of column based on which some target values will be set to nan to avoid leakage\n",
    "#        param6 (exclude_values): list of values defining the rows for which the target column should be set to nan\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.\n",
    "    \n",
    "    df_nan = df.copy()\n",
    "    df_nan.loc[df_nan[exclude_column].isin(exclude_values),target_name] = np.nan\n",
    "    group = df_nan.groupby(cols_to_encode)[target_name].agg(encoding_func).reset_index()\n",
    "    new_col_names = ['_'.join(cols_to_encode) + '_' + func.__name__ + '_encod' for func in encoding_func]\n",
    "    group.columns = cols_to_encode + new_col_names\n",
    "    df = pd.merge(df, group, on=cols_to_encode, how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def target_encoding_calculation(input_df, target_name, to_encode, encoding_functions, exclude_column, exclude_values):\n",
    "    df = input_df.copy()\n",
    "    for COLS in to_encode:\n",
    "        df = target_encoding(df = df,\n",
    "                             target_name = target_name,\n",
    "                             cols_to_encode = COLS,\n",
    "                             encoding_func = encoding_functions,\n",
    "                             exclude_column = exclude_column,\n",
    "                             exclude_values = exclude_values)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME = 'sales'\n",
    "ENCODING_FUNCTIONS = [np.nanmean, np.nanstd]\n",
    "EXCLUDE_COLUMN = 'date_block_num_day'\n",
    "EXCLUDE_VALUES = DAYS_VALID1 + DAYS_VALID2 + DAYS_VALID3 + DAYS_VALID4 + DAYS_VALID5 + DAYS_PRED\n",
    "\n",
    "to_encode = [ ['item_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step target_encoding_calculation took 0:00:01.505591s - 2022-05-24 15:37:11.868689 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 71.07 Mb (27.2% reduction)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4652320 entries, 0 to 4652319\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   item_id_nanmean_encod  4652320 non-null  float16\n",
      " 1   item_id_nanstd_encod   4652320 non-null  float16\n",
      "dtypes: float16(2)None\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['sales','state_id','store_id','cat_id','dept_id','item_id']\n",
    "input_df = get_input_df(input_cols)\n",
    "\n",
    "features_cat_encoding = input_df.pipe(target_encoding_calculation, TARGET_NAME, to_encode, ENCODING_FUNCTIONS, EXCLUDE_COLUMN, EXCLUDE_VALUES) \\\n",
    "                                .pipe(feature_formatting, input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic target encoding for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target encoding done just above is static : the average of sales is done on the whole dataset, at any time. A dynamic target encoding might be more accurate : it only takes into account sales that occured before the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def target_encoding_dynamic(df, target_name:str, cols_to_encode:list, date_column:str):\n",
    "\n",
    "#    Function encoding a categorical feature (or a group of categorical features) based on aggregated target values.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (target_name): name of target column\n",
    "#        param3 (cols_to_encode): list of the names of the columns to encode\n",
    "#        param4 (date_column): name of date column\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.\n",
    "    \n",
    "    # This function is only used to avoid getting \"inf\" when sums become too large\n",
    "    def sum_div100000(x):\n",
    "        return 0.00001 * sum(x)\n",
    "    \n",
    "    group = df.groupby(cols_to_encode+[date_column])[target_name].agg(['count',sum_div100000]).reset_index().dropna(subset=['sum_div100000'])\n",
    "    cumsums = group.groupby(cols_to_encode).cumsum()[['count','sum_div100000']]\n",
    "    cumsums.columns = ['cum_count','cum_sum']\n",
    "    group2 = group.merge(cumsums, how='left', left_index=True, right_index=True)\n",
    "    group2['_'.join(cols_to_encode) + '_cum_mean_encod'] = 100000 * group2['cum_sum'] / group2['cum_count']\n",
    "    group2 = group2[[date_column] + cols_to_encode + ['_'.join(cols_to_encode) + '_cum_mean_encod']]\n",
    "    df = df.merge(group2, how='left', on=[date_column]+cols_to_encode)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def target_encoding_dynamic_calculation(input_df, target_name, date_column, to_encode):\n",
    "    df = input_df.copy()\n",
    "    for COLS in to_encode:\n",
    "        df = target_encoding_dynamic(df = df,\n",
    "                                     target_name = target_name,\n",
    "                                     cols_to_encode = COLS,\n",
    "                                     date_column = date_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME = 'sales'\n",
    "DATE_COLUMN = 'date_block_num_day'\n",
    "\n",
    "to_encode = [ ['item_id'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step target_encoding_dynamic_calculation took 0:00:31.623542s - 2022-05-24 15:37:14.169961 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 62.20 Mb (30.0% reduction)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4652320 entries, 0 to 4652319\n",
      "Data columns (total 1 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   item_id_cum_mean_encod  4568320 non-null  float16\n",
      "dtypes: float16(1)None\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['sales','state_id','store_id','cat_id','dept_id','item_id']\n",
    "input_df = get_input_df(input_cols)\n",
    "\n",
    "features_cat_encoding_dyn = input_df.pipe(target_encoding_dynamic_calculation, TARGET_NAME, DATE_COLUMN, to_encode) \\\n",
    "                                    .pipe(feature_formatting, input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar events encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested in several Kaggle discussions, the columns `event_name_1` and `event_name_2` are encoded as follows. One column is created for each type of event (ex: Christmas, Thanksgiving...). For each event, the feature is :\n",
    "\n",
    "- Negative the previous 25 days (-25, -24, .. -1)\n",
    "- Equal to 0 on the day of the event\n",
    "- Positive for the next 25 days (1.2, .. 25)\n",
    "- NaN otherwise\n",
    "\n",
    "The objective is to model the impact of the event on sales before and after D-day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_step\n",
    "def calendar_events_encoding(df, event_columns:list, date_col_name:str, days_range:list):\n",
    "\n",
    "#    Function encoding events by adding one column by event to the orginal dataframe.\n",
    "#    The value of the feature is negative before the event, equal to 0 the day of the event, and positive after.\n",
    "#    Args:\n",
    "#        param1 (df): a pandas dataframe\n",
    "#        param2 (event_columns): list of columns to be encoded, whose values are events names\n",
    "#        param3 (date_col_name): name of date column, where date is coded as an integer\n",
    "#        param4 (days_range): list of integer defining the range of days taking negative or positive values around the event\n",
    "#    Returns:\n",
    "#        df: the input dataframe with nice features.\n",
    "    \n",
    "    all_events = list()\n",
    "    for col in event_columns:\n",
    "        all_events += df[col].dropna().unique()\n",
    "    all_events = set(all_events)\n",
    "    \n",
    "    for event in all_events:\n",
    "        \n",
    "        event_days = list()\n",
    "        for col in event_columns:\n",
    "            event_days += list(df.loc[df[col] == event][date_col_name])\n",
    "        event_days = set(event_days)\n",
    "            \n",
    "        for i in DAYS_RANGE:\n",
    "            days = [d+i for d in event_days]\n",
    "            df.loc[df[date_col_name].isin(days),event] = i\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_COLS = ['event_name_1','event_name_2']\n",
    "DATE_COL = 'date_block_num_day'\n",
    "DAYS_RANGE = range(-25,26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: -- Step calendar_events_encoding took 0:02:12.948402s - 2022-05-24 15:46:07.632562 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 284.04 Mb (73.8% reduction)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4652320 entries, 0 to 4652319\n",
      "Data columns (total 30 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   Cinco De Mayo        725855 non-null  float16\n",
      " 1   PresidentsDay        691154 non-null  float16\n",
      " 2   SuperBowl            667423 non-null  float16\n",
      " 3   ValentinesDay        680876 non-null  float16\n",
      " 4   Pesach End           719821 non-null  float16\n",
      " 5   Ramadan starts       705933 non-null  float16\n",
      " 6   Eid al-Fitr          599891 non-null  float16\n",
      " 7   NBAFinalsStart       709756 non-null  float16\n",
      " 8   LentWeek2            702969 non-null  float16\n",
      " 9   LaborDay             603637 non-null  float16\n",
      " 10  NewYear              620818 non-null  float16\n",
      " 11  Christmas            619468 non-null  float16\n",
      " 12  StPatricksDay        707828 non-null  float16\n",
      " 13  LentStart            699060 non-null  float16\n",
      " 14  IndependenceDay      588259 non-null  float16\n",
      " 15  Thanksgiving         615547 non-null  float16\n",
      " 16  ColumbusDay          609986 non-null  float16\n",
      " 17  Easter               718781 non-null  float16\n",
      " 18  NBAFinalsEnd         661614 non-null  float16\n",
      " 19  Halloween            612961 non-null  float16\n",
      " 20  Father's day         661355 non-null  float16\n",
      " 21  MartinLutherKingDay  625092 non-null  float16\n",
      " 22  Mother's day         727523 non-null  float16\n",
      " 23  Chanukah End         618144 non-null  float16\n",
      " 24  OrthodoxEaster       722546 non-null  float16\n",
      " 25  MemorialDay          716429 non-null  float16\n",
      " 26  VeteransDay          614206 non-null  float16\n",
      " 27  Purim End            705560 non-null  float16\n",
      " 28  OrthodoxChristmas    622075 non-null  float16\n",
      " 29  EidAlAdha            611603 non-null  float16\n",
      "dtypes: float16(30)None\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['event_name_1','event_name_2']\n",
    "input_df = get_input_df(input_cols)\n",
    "\n",
    "features_events_encoding = input_df.pipe(calendar_events_encoding, EVENT_COLS, DATE_COL, DAYS_RANGE) \\\n",
    "                                   .pipe(feature_formatting, input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all sub dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features have been stored in different dataframes. Let's merge them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = [features_time, features_cum_mean, features_prices, features_lags, features_lags_agg, \n",
    "          features_cat_encoding, features_cat_encoding_dyn, features_events_encoding]\n",
    "index_cols = ['id','date_block_num_day']\n",
    "df_final = reduce(lambda left, right : merge_by_concat(left, right, index_cols), all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows and columns removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the first year because the lags are not calculated properly on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date_lag = df_final[\"date\"].min() + timedelta(366)\n",
    "df_final = df_final.loc[df_final[\"date\"] > min_date_lag, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we drop useless columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_to_drop = ['id','year_month','wm_yr_wk','event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "df_final = df_final.drop(list_col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id              category\n",
       "dept_id              category\n",
       "cat_id               category\n",
       "store_id             category\n",
       "state_id             category\n",
       "                       ...   \n",
       "MemorialDay           float16\n",
       "VeteransDay           float16\n",
       "Purim End             float16\n",
       "OrthodoxChristmas     float16\n",
       "EidAlAdha             float16\n",
       "Length: 145, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df_final, \"train_df_final_\" + \".pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
